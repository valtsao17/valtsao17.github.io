<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://valtsao17.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://valtsao17.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-11T00:37:26+00:00</updated><id>https://valtsao17.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">understanding the math behind VAEs</title><link href="https://valtsao17.github.io/blog/2025/kl/" rel="alternate" type="text/html" title="understanding the math behind VAEs"/><published>2025-08-09T00:32:13+00:00</published><updated>2025-08-09T00:32:13+00:00</updated><id>https://valtsao17.github.io/blog/2025/kl</id><content type="html" xml:base="https://valtsao17.github.io/blog/2025/kl/"><![CDATA[<p>In VAEs (as well as many other ML models), the Kullback-Leibler (KL) divergence is used to measure the “closeness” between two probability distributions. Attempting to understand this basic concept in a deeper manner, here I derive the closed-form expression for it between two Gaussians, where \(q(z)\) represents the approximate posterior and \(p(z)\) represents the prior.</p> <hr/> <p>Assume that they are univariate (one dimension) for simplicity, and simplify this form: \(KL(q(z) || p(z))) = E_{q(z)} \left[log\dfrac{q(z)}{p(z)}\right] = \int_z q(z) log \dfrac{q(z)}{p(z)}dz,\) or, \(KL(N(\mu_q,\sigma_q^2)||N(\mu_p, \sigma_p^2)).\)</p> <p>Let’s start with the definition of the KL divergence, given as: \(\begin{equation*} KL(q(z) || p(z))) = E_{q(z)} \left[log\dfrac{q(z)}{p(z)}\right] = \int_z q(z) log \dfrac{q(z)}{p(z)}dz \tag{1} \end{equation*}\)</p> <p>We know that \(p(z)\) and \(q(z)\) are normal distributions with general form of their PDF being \(\mathcal{N} (z; \mu, \sigma^2) = \dfrac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\dfrac{(z-\mu)^2}{2\sigma^2} \right).\)</p> <p>Now, we plug in the \(\mu\) and \(\sigma^2\) of \(p(z)\) and \(q(z)\) and take their ratio in order to try and match the form of the KL divergence in Eq. (1). Thus, we get: \(\begin{equation*} \frac{q(z)}{p(z)} = \frac{\frac{1}{\sqrt{2\pi\sigma_q^2}} \exp\left( -\frac{(z-\mu_q)^2}{2\sigma_q^2} \right)}{\frac{1}{\sqrt{2\pi\sigma_p^2}} \exp\left( -\frac{(z-\mu_p)^2}{2\sigma_p^2} \right)} = \sqrt{\frac{\sigma_p^2}{\sigma_q^2}} \exp\left( -\frac{(z-\mu_q)^2}{2\sigma_q^2} + \frac{(z-\mu_p)^2}{2\sigma_p^2} \right). \tag{2} \end{equation*}\)</p> <p>Taking the logarithm of Eq.(2) yields \(\log\dfrac{q(z)}{p(z)} = \dfrac{1}{2} \log \dfrac{\sigma_p^2}{\sigma_q^2} - \dfrac{(z-\mu_q)^2}{2\sigma_q^2} + \dfrac{(z-\mu_p)^2}{2\sigma_p^2}.\)</p> <p>Because we want to attain the KL divergence form, we need to integrate over the real numbers of the distribution of \(q(z)\). This is equivalent to taking the expectation with respect to \(q(z)\). Doing this results in the final form \(\begin{equation*} \text{KL}(q(z) \| p(z)) = \dfrac{1}{2} \log \dfrac{\sigma_p^2}{\sigma_q^2} + \dfrac{1}{2} \left[\dfrac{\mathbb{E}_q [ (z-\mu_p)^2]}{\sigma_p^2} - \dfrac{\mathbb{E}_q [ (z-\mu_q)^2]}{\sigma_q^2}\right]. \tag{3} \end{equation*}\)</p> <p>From the definition of variance, we know that: \(\begin{equation*} \mathbb{E}_q [ (z-\mu_q)^2] = \sigma_q^2 \tag{4} \end{equation*}\)</p> <p>In addition, because $q(z)$ follows a normal distribution given by \(\mathcal{N}(\mu_q, \sigma_q^2)\), we can simplify as follows: \(\begin{align*} \mathbb{E}_q [ (z-\mu_p)^2] &amp;= \mathbb{E}_q [ (z^2 - 2z\mu_p + \mu_p^2] \\ &amp;= \mathbb{E}_q [z^2] - 2\mu_p \mathbb{E}_q [z] + \mathbb{E}_q [\mu_p^2] \\ &amp;= \sigma_q^2 + \mu_q^2 - 2\mu_p \mu_q + \mu_p^2 \\ &amp;= \sigma_q^2 + (\mu_q - \mu_p)^2 \tag{5} \end{align*}\)</p> <p>Plugging Eq.(4) &amp; Eq.(5) into Eq. (3) and simplifying, we get the desired closed form expression: \(\begin{align*} \boxed{\text{KL}(q(z) \| p(z)) = \dfrac{1}{2} \log \dfrac{\sigma_p^2}{\sigma_q^2} + \dfrac{1}{2} \left[\dfrac{\sigma_q^2 + (\mu_q - \mu_p)^2}{\sigma_p^2} - 1 \right].} \tag{6} \end{align*}\)</p>]]></content><author><name></name></author><category term="ml"/><category term="ml"/><category term="theory"/><summary type="html"><![CDATA[deriving KL-divergence]]></summary></entry></feed>