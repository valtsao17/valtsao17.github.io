<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://valtsao17.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://valtsao17.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-08-31T16:39:39+00:00</updated><id>https://valtsao17.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">understanding the math behind VAEs</title><link href="https://valtsao17.github.io/blog/2025/kl/" rel="alternate" type="text/html" title="understanding the math behind VAEs"/><published>2025-08-09T00:32:13+00:00</published><updated>2025-08-09T00:32:13+00:00</updated><id>https://valtsao17.github.io/blog/2025/kl</id><content type="html" xml:base="https://valtsao17.github.io/blog/2025/kl/"><![CDATA[<p>In VAEs (as well as many other ML models), the Kullback-Leibler (KL) divergence is used to measure the ‚Äúcloseness‚Äù between two probability distributions. Attempting to understand this basic concept in a deeper manner, here I derive the closed-form expression for it between two Gaussians, where \(q(z)\) represents the approximate posterior and \(p(z)\) represents the prior.</p> <hr/> <p>Assume that they are univariate (one dimension) for simplicity, and simplify this form:</p> \[KL(q(z) || p(z))) = E_{q(z)} \left[log\dfrac{q(z)}{p(z)}\right] = \int_z q(z) log \dfrac{q(z)}{p(z)}dz,\] <p>or,</p> \[KL(N(\mu_q,\sigma_q^2)||N(\mu_p, \sigma_p^2)).\] <p>Let‚Äôs start with the definition of the KL divergence, given as:</p> \[\begin{equation*} KL(q(z) || p(z))) = E_{q(z)} \left[log\dfrac{q(z)}{p(z)}\right] = \int_z q(z) log \dfrac{q(z)}{p(z)}dz \tag{1} \end{equation*}\] <p>We know that \(p(z)\) and \(q(z)\) are normal distributions with general form of their PDF being</p> \[\mathcal{N} (z; \mu, \sigma^2) = \dfrac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\dfrac{(z-\mu)^2}{2\sigma^2} \right).\] <p>Now, we plug in the \(\mu\) and \(\sigma^2\) of \(p(z)\) and \(q(z)\) and take their ratio in order to try and match the form of the KL divergence in Eq. (1). Thus, we get: \(\begin{equation*} \frac{q(z)}{p(z)} = \frac{\frac{1}{\sqrt{2\pi\sigma_q^2}} \exp\left( -\frac{(z-\mu_q)^2}{2\sigma_q^2} \right)}{\frac{1}{\sqrt{2\pi\sigma_p^2}} \exp\left( -\frac{(z-\mu_p)^2}{2\sigma_p^2} \right)} = \sqrt{\frac{\sigma_p^2}{\sigma_q^2}} \exp\left( -\frac{(z-\mu_q)^2}{2\sigma_q^2} + \frac{(z-\mu_p)^2}{2\sigma_p^2} \right). \tag{2} \end{equation*}\)</p> <p>Taking the logarithm of Eq.(2) yields</p> \[\log\dfrac{q(z)}{p(z)} = \dfrac{1}{2} \log \dfrac{\sigma_p^2}{\sigma_q^2} - \dfrac{(z-\mu_q)^2}{2\sigma_q^2} + \dfrac{(z-\mu_p)^2}{2\sigma_p^2}.\] <p>Because we want to attain the KL divergence form, we need to integrate over the real numbers of the distribution of \(q(z)\). This is equivalent to taking the expectation with respect to \(q(z)\). Doing this results in the final form \(\begin{equation*} \text{KL}(q(z) \| p(z)) = \dfrac{1}{2} \log \dfrac{\sigma_p^2}{\sigma_q^2} + \dfrac{1}{2} \left[\dfrac{\mathbb{E}_q [ (z-\mu_p)^2]}{\sigma_p^2} - \dfrac{\mathbb{E}_q [ (z-\mu_q)^2]}{\sigma_q^2}\right]. \tag{3} \end{equation*}\)</p> <p>From the definition of variance, we know that: \(\begin{equation*} \mathbb{E}_q [ (z-\mu_q)^2] = \sigma_q^2 \tag{4} \end{equation*}\)</p> <p>In addition, because \(q(z)\) follows a normal distribution given by \(\mathcal{N}(\mu_q, \sigma_q^2)\), we can simplify as follows: \(\begin{align*} \mathbb{E}_q [ (z-\mu_p)^2] &amp;= \mathbb{E}_q [ (z^2 - 2z\mu_p + \mu_p^2] \\ &amp;= \mathbb{E}_q [z^2] - 2\mu_p \mathbb{E}_q [z] + \mathbb{E}_q [\mu_p^2] \\ &amp;= \sigma_q^2 + \mu_q^2 - 2\mu_p \mu_q + \mu_p^2 \\ &amp;= \sigma_q^2 + (\mu_q - \mu_p)^2 \tag{5} \end{align*}\)</p> <p>Plugging Eq.(4) &amp; Eq.(5) into Eq. (3) and simplifying, we get the desired closed form expression: \(\begin{align*} \boxed{\text{KL}(q(z) \| p(z)) = \dfrac{1}{2} \log \dfrac{\sigma_p^2}{\sigma_q^2} + \dfrac{1}{2} \left[\dfrac{\sigma_q^2 + (\mu_q - \mu_p)^2}{\sigma_p^2} - 1 \right].} \tag{6} \end{align*}\)</p> <p>Assuming that \(ùëù(ùëß)\) is \(ùëÅ(0,1)\) for simplicity, we can show that this simplifies to:</p> \[\begin{align*} KL(N(\mu_q, \sigma_q^2)||N(0,1)) = \frac{1}{2}\sigma_q^2 + \mu_q^2 -1 -\log\sigma_q^2. \end{align*}\] <p>For a standard normal distribution where \(p(z)\) is \(\mathcal{N}(0,1)\), we know that \(\mu_p=0\) and \(\sigma_p^2=1\). Plugging these into Eq.(6) above, we get:</p> \[\begin{align} \text{KL}(q(z) \mid\mid p(z)) = \dfrac{1}{2} \log \dfrac{1}{\sigma_q^2} + \dfrac{1}{2} (\sigma_q^2 + (\mu_q-0)^2 -1). \tag{7}\\ \end{align}\] <p>Since we know \(\log \dfrac{1}{\sigma_q^2} = -\log \sigma_q^2\), we can simplify Eq.(1) and obtain \(\begin{align} \boxed{\text{KL}(q(z) \| p(z)) = \dfrac{1}{2} (\sigma_q^2 + \mu_q^2 -1 - \log\sigma_q^2).} \tag{8}\\ \end{align}\)</p> <p>We know that the KL divergence is always non-negative, so it must be minimized when it‚Äôs equal to 0. In our case, using the expression derived in Eq.(8):</p> \[\text{KL}(\mathcal{N}(\mu_q,\sigma_q^2) \| \mathcal{N}(0,1))) = \dfrac{1}{2} (\sigma_q^2 + \mu_q^2 -1 - \log\sigma_q^2),\] <p>we notice that this is minimized precisely when</p> \[\sigma_q^2 =1 \quad \text{and} \quad \mu_q = 0,\] <p>which would give us a divergence of 0‚Äìin other words, when \(q(z)\) is a standard normal distribution. Therefore, \(q(z)\) must equal \(p(z)\).</p>]]></content><author><name></name></author><category term="ml"/><category term="ml"/><category term="theory"/><summary type="html"><![CDATA[deriving KL-divergence]]></summary></entry></feed>