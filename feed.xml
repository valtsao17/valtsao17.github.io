<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://valtsao17.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://valtsao17.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-11-25T16:27:31+00:00</updated><id>https://valtsao17.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">intro to polynomial chaos expansion (pce)</title><link href="https://valtsao17.github.io/blog/2025/pce/" rel="alternate" type="text/html" title="intro to polynomial chaos expansion (pce)"/><published>2025-11-25T00:32:13+00:00</published><updated>2025-11-25T00:32:13+00:00</updated><id>https://valtsao17.github.io/blog/2025/pce</id><content type="html" xml:base="https://valtsao17.github.io/blog/2025/pce/"><![CDATA[ <h2 id="introduction">Introduction</h2> <p>This section is more meant for me to summarize some of the things I read, as well as put some thought into digesting these concepts on my own. A lot of the ideas I jot down here are because I thought them necessary to know in order to complete the exercises that follow. Please feel free to skip over this section and proceed to the solutions below.</p> <p>Polynomial chaos expansion (PCE) is a way for us to represent random variables in terms of (orthogonal) polynomials, similar to a Taylor/Maclaurin series or a Fourier series. This series gets truncated in practice to get accurate approximations that converge much faster than typical Monte Carlo simulations. They also allow for closed-form formulas for mean, variance, etc. unlike other forms of decompositions/expansions. Primarily, this has been used in uncertainty quantification and sensitivty analysis applications to easily summarize distributions that may have weird forms. Unlike in Principal Component Analysis (PCA), where the basis is data-dependent (ie. the eigenvectors of a covariance matrix), the basis for PCE can be chosen a-priori according to the input distribution.</p> <h2 id="some-preliminaries-and-notes">Some preliminaries and notes</h2> <h3 id="orthogonality">Orthogonality</h3> <p>Before diving into PCE, it‚Äôs important for us to briefly review some key concepts that will act as the building blocks of our understanding later. Let‚Äôs take two random variables $X$ and $Y$ with finite variance. We define the <strong>inner product</strong> between the two as</p> \[&lt;X,Y&gt; \, = \mathbb{E}[XY].\] <p>This is just like the dot product for vectors, but for random variables.</p> <ul> <li>We say two random variables \(X\) and \(Y\) are <strong>orthogonal</strong> if \(\mathbb{E}[XY]=0.\)</li> <li>This is also called <strong>independence</strong>; recall that orthogonality between two vector represents a right angle between the two. This interpretation works in \(\mathbb{R}^n\), but in more general spaces (i.e. Hilbert spaces), this sort of geometric intuition falls apart.</li> </ul> <p>Now recall that the \(L^2\) norm of a random variable \(X\) is</p> \[||X||_{L^2} = \sqrt{\mathbb{E}[X^2]}.\] <ul> <li>This is important because the \(L^2\) norm we can say represents the energy of a random variable. Minimizing the \(L^2\) error means that we find the best approximation of the original random variable on average. In other words, rather than working directly with the random variable, we often times in probability work with the \(L^2\) norm of the random variable.</li> </ul> <p>When we approximate a random variable \(Y\) by a truncated series \(\hat{Y}\), the \(L^2\) error is</p> \[\text{Error}= ||Y - \hat{Y}||_{L^2} = \sqrt{\mathbb{E}[(Y - \hat{Y})^2]}.\] <p>This is exactly the <strong>root mean square error (RMSE).</strong></p> <h3 id="pce-minimizes-l2-error">PCE Minimizes \(L^2\) Error</h3> <p>One way to think of the \(L^2\) norm is to see it as measuring the <em>size</em> of a random variable. What we mean by this is that it gives us a way to answer the question ‚Äúhow far from zero is this random variable on average?‚Äù As a way to penalize large errors more heavily, we square the random variable and thus the value is always positive.</p> <blockquote> <p><strong>Example</strong> <br/> Suppose we‚Äôre measuring temperature fluctuations: \(X\) can be -2, 0, +2, with equal probability.</p> <ul> <li>The \(L^2\) norm is \(\|X\|_{L^2} = \sqrt{\tfrac{1}{3}((-2)^2 + 0^2 + 2^2)} = \sqrt{\tfrac{8}{3}} \approx 1.63\).</li> <li>This tells us the ‚Äútypical‚Äù temperature deviation is about 1.63 degrees.</li> </ul> <p>If we approximate \(X\) with \(\hat{X}=0\) (always predicting zero), the error is \(\|X - \hat{X}\|_{L^2} = \|X\|_{L^2} \approx 1.63\). A better approximation would give a smaller \(L^2\)error.</p> </blockquote> <p>The key to understanding why PCE minimizes the \(L^2\) error is that orthogonal basis functions let us find the best coefficients independently. Let‚Äôs clarify this a bit more. Say we want to approximate some random variable \(Y\) using a combination of polynomial basis functions \(\Psi_0, \Psi_1, \Psi_2, \dots\):</p> \[Y \approx \hat{Y} = c_0 \Psi_0 + c_1 \Psi_1 + c_2 \Psi_2 + \dots + c_P \Psi_P.\] <p>What coefficients \(c_1\) make \(\hat{Y}\) as close as possible to \(Y\)? To answer this question, we try to minimize the \(L^2\) error of \(\hat{Y}\): \(\mathbb{E}[(Y-\hat{Y})^2]\). We expand this error to get</p> \[\mathbb{E}[(Y-\hat{Y})^2] = \mathbb{E} \left[ \left(Y - \sum_{i=0}^{P} c_i \Psi_i \right)^2 \right] \\ = \mathbb{E}[Y^2] - 2\sum_{i=0}^{P} c_i \mathbb{E}[Y \Psi_i] + \sum_{i=0}^P c_i^2 \mathbb{E}[\Psi_i^2] + \sum_{i \neq j}c_i c_j \mathbb{E}[\Psi_i \Psi_j].\] <p>Because we want this quantity to be as small as possible, notice that the way we can make it as \textit{small as possible} is if the cross-terms \(\mathbb{E}[\Psi_i \Psi_j\) were zero! And if \(\mathbb{E}[\Psi_i \Psi_j]=0\), then by definition \(\Psi_i\) and \(\Psi_j\) are orthogonal. Hence our quantity becomes</p> \[\mathbb{E}[(Y-\hat{Y})^2] = \mathbb{E}[Y^2] - 2\sum_{i=0}^{P} c_i \mathbb{E}[Y \Psi_i] + \sum_{i=0}^P c_i^2 \mathbb{E}[\Psi_i^2].\] <p>To minimize this with respect to each $c_i$, we take the derivation and set it to zero:</p> \[\frac{\partial}{\partial c_i} \mathbb{E}[(Y - \hat{Y})^2] = -2 \mathbb{E}[Y \Psi_i] + 2c_i \mathbb{E}[\Psi_i^2] = 0.\] <p>This allows us to reach the following formula:</p> \[\boxed{c_i = \frac{\mathbb{E}[Y \Psi_i]}{\mathbb{E}[\Psi_i^2]}.}\] <p>Notice that each coefficient can be computed independently (thanks to orthogonality), and this choice of coefficients automatically minimizes the \(L^2\) error. If the polynomials weren‚Äôt orthogonal, we‚Äôd have to solve coupled equations for all coefficients simultaneously‚Ä¶which is much harder and less accurate.</p> <h3 id="orthogonal-polynomials">Orthogonal polynomials</h3> <p>Now we understand that PCE minimizes the \(L^2\) error. So, why do we need <em>orthogonal</em> polynomials? Let‚Äôs say we were trying to approximate a function using a basis of vectors, like sines and cosines in Fourier series. If our basis vectors are orthogonal, computing coeÔ¨Écients is easy; we can just project onto each basis element independently. However, if they‚Äôre not orthogonal, then we‚Äôd need to solve a system of equations, which is very computationally expensive, and doesn‚Äôt always give the unique best coefficients. The same principle applies here, with the added fact that orthogonality depends on the probability distribution now.</p> <p>When polynomials are orthogonal, this means that</p> \[&lt;P_n, P_m&gt; = \int P_n(\xi) P_m(\xi) \rho(\xi) d\xi = 0 \quad \text{for} \quad n \neq m,\] <p>where \(\rho(\xi)\) is the probability density function (PDF). From this, we can see that even with the same functions but different distributions, we can get different orthogonality results.</p> <p>There are a few classical orthogonal polynomials, like the Hermite polynomials (for Gaussian distribution), the Laguerre polynomials (for Gamma distribution), and the Jacobi polynomials (for Beta distribution). \cite{classicalpolynomials} If we are working with a distribution that doesn‚Äôt match one of the named few, often we need to perform a transformation of random variables (i.e. via an inverse CDF transform) to one that does match (for example, \(U [a, b] \mapsto U [-1, 1]\)).</p> <p><strong>Legendre polynomials</strong> A special case of the Jacobi is the Legendre polynomials, which are associated with the Uniform distribution on \([-1,1]\). They are the natural basis for functions defined in an interval with no preference for any particular value.</p> <p>Speaking of which, our first exercise is defined on the Uniform distribution! The first few Legendre polynomials are defined by:</p> \[\begin{aligned} P_0(x) &amp;= 1, \\ P_1(x) &amp;= x, \\ P_2(x) &amp;= \frac{1}{2}(3x^2 - 1), \\ P_3(x) &amp;= \frac{1}{2}(5x^3 -3x), \\ P_4(x) &amp;= \frac{1}{8}(35x^4 - 30x^2 + 3), \\ &amp;\vdots \end{aligned}\] <p>Rigorously, Legendre polynomials are orthogonal with respect to the uniform measure on \([-1, 1]\):</p> \[\int_{-1}^{1} P_n(x) P_m(x) dx = \frac{2}{2n+1} \delta_{nm}.\] <p>They can be calculated by the following recurrence relation:</p> \[(n+1)P_{n+1}(x) = (2n+1)xP_n(x) - nP_{n-1}(x),\] <p>with \(P_0(x)=1\) and \(P_1(x)=x\). Alternatively though, they can be defined using Rodrigues‚Äô Formula, which gives us:</p> \[P_n(x) = \frac{1}{2^n n!} \frac{d^n}{dx^n}[(x^2-1)^n].\] <p><strong>Hermite polynomials</strong> Another set of orthogonal polynomials are the Hermite polynomials, associated with the standard Normal distribution \(\mathcal{N}(0,1)\). They are the natural basis for Gaussian randomness, which is quite prevalent due to the Central Limit Theorem.</p> <p>For the ensuing notes, we will follow the probabilist‚Äôs Hermite polynomials, \(\text{He}_n\) as opposed to the physicist‚Äôs \(H_n\),\cite{garfken67:math} taking note that there does indeed exist two conventions. The first few Hermite polynomials are:</p> \[\begin{align*} \text{He}_0(x) &amp;= 1, \\ \text{He}_1(x) &amp;= x, \\ \text{He}_2(x) &amp;= x^2-1, \\ \text{He}_3(x) &amp;= x^3 -3x, \\ \text{He}_4(x) &amp;= x^4 - 6x^2 +3, \\ \text{He}_5(x) &amp;= x^5 -10x^3 + 15x, \\ &amp;\vdots \end{align*}\] <p>We see here that each polynomials has degree $n$ and removes the mean of $x^n$ under the Gaussian measure:</p> \[\mathbb{E}[\text{He}_n(\xi)] =0 \quad \text{for} \,\, \xi \sim \mathcal{N}(0,1) \,\, \text{and} \,\, n \geq 1.\] <p>Rigorously, Hermite polynomials are orthogonal with respect to the standard Gaussian measure:</p> \[\mathbb{E}[\text{He}_n(\xi) \text{He}_m(\xi)] = \int_{-\infty}^\infty \text{He}_n (x) \text{He}_m (x) \frac{e^{-x^2/2}}{\sqrt{2\pi}}dx = n! \cdot \delta_{nm}\] <p>Note the normalization: \(\mathbb{E}[\text{He}_n^2] = n!\) (meaning factorial growth). We calculate them with the following recurrence relation:</p> \[\text{He}_{n+1}(x) = x \cdot \text{He}_n (x) -n \cdot \text{He}_{n-1} (x),\] <p>with \(\text{He}_0 (x) =1\) and \(\text{He}_1 (x) =x\). Using Rodrigues‚Äô Formula again, we can find</p> \[\text{He}_n(x) = (-1)^n e^{x^2/2} \frac{d^n}{dx^n}e^{-x^2/2}.\] <p>There exists deeper connections to stochastic theory as for why Hermite polynomials represent Gaussians, but we won‚Äôt get into that here. For more information, see the Focker-Plank equation,\cite{10.1090/S0025-5718-01-01365-5, khasi2022numerical} Wiener chaos,\cite{Russo2022} and optimal basis.\cite{hermitebasis}</p> <p><strong>Key Property:</strong> If \(\xi\) follows distribution with density \(\rho(\xi)\), then the polynomials \({P_n(\xi)}\) satisfy</p> \[\int P_n(\xi) P_m(\xi) \rho(\xi) d\xi = h_n \delta_{nm}\] <p>where \(\delta_{nm}\) is the Kronecker delta and $h_n$ is a normalization constant.</p> <blockquote> <p><strong>Main Ideas</strong></p> <ul> <li>For Legendre polynomials, the normalization constants are:</li> </ul> \[h_n = \int_{-1}^{1} P_n(\xi)\, P_m(\xi)\, d\xi = \frac{2}{2n + 1}.\] <p><em>Note:</em> this assumes \(U[-1,1]\) (uniform on ([-1,1]))!<br/> If not the case, we need to multiply by the PDF of the uniform distribution.</p> <ul> <li>For Hermite polynomials, the normalization constants are:</li> </ul> \[h_n = \int_{-\infty}^{\infty} \mathrm{He}_n(\xi)\, \mathrm{He}_m(\xi)\, d\xi = \sqrt{2\pi}\,n!.\] <p>Note: this assumes \(\mathcal{N}(0,1)\). For non-standard Gaussian, we‚Äôd need to multiply by another normalization constant.</p> </blockquote> <h3 id="polynomial-chaos-expansion">Polynomial Chaos Expansion</h3> <blockquote> <p><strong>Definition</strong></p> \[Y = \sum_{t=0}^\infty c_i \Psi_i (\xi),\] <p>where \(\Psi_i (\xi)\) are orthogonal polynomials with respect to the distribution of \(\xi\), and \(c_i\) are deterministic coefficients.</p> </blockquote> <hr/> <p><strong>The coefficient formula.</strong></p> <p>To compute the coefficients, we use orthogonality:</p> \[c_i=\frac{\mathbb{E}[Y\Psi_i(\xi)]}{\mathbb{E}[\Psi_i^2(\xi)]}.\] <p>In practice, we truncate to order \(P\):</p> \[Y \approx \sum_{i=0}^P c_i \Psi_i (\xi).\] <hr/> <h3 id="computing-statistics-using-pces">Computing statistics using PCEs</h3> <p>Once we have the expansion</p> \[Y \approx \sum_{i=0}^P c_i \Psi_i (\xi),\] <p>the mean easily follows:</p> \[\mathbb{E}[Y]=c_0.\] <p>This is because \(\Psi_0 = 1\) and all other polynomials have zero mean. The variance is:</p> \[\mathrm{Var}(Y) = \sum_{i=1}^{P}c_i^2 \,\mathbb{E}[\Psi_i^2].\] <hr/> <h3 id="computing-coefficients-in-practice">Computing coefficients in practice</h3> <p>There exist two main approaches to computing our coefficients: the projection method and the Galerkin method.</p> <ul> <li> <p>The non-instrusive method involves computing the expected values numerically:</p> \[c_i = \frac{1}{\mathbb{E}[\Psi_i^2]} \int Y(\xi)\,\Psi_i(\xi)\,\rho(\xi)\,d\xi \approx \frac{1}{\mathbb{E}[\Psi_i^2]} \sum_{k=1}^{N_q} Y(\xi_k)\,\Psi_i(\xi_k)\,w_k,\] <p>where \(\xi_k\) are quadrature points and \(w_k\) are weights. This is called non-intrusive because we do not modify the governing equations whatsoever. It does, however, suffer from the curse of dimensionality, so when working in high dimensions this often becomes too expensive.</p> </li> <li> <p>The Galerkin method involves substituting the expansion directly into the governing equations and projecting onto each basis function. This is called intrusive because we must modify/approximate the governing equations, and so it no longer strictly enforces them.</p> </li> </ul> <p>For linear ODEs, these two methods are equivalent. For nonlinear ODEs, the truncation errors differ. The Galerkin approach propagates uncertainty more accurately but requires computing expectation of nonlinear terms.</p> <hr/> <h2 id="exercises">Exercises</h2> <blockquote> <p><strong>Problem 1</strong></p> <p>Let \(X \sim\) Uniform[-1,1] and \(Y=\sin(\pi X)\).</p> <p>(a) Expand \(Y\) using PCE.<br/> (b) Determine \(\mathbb{E}[Y]\) and \(\text{Var}(Y)\) from the expansion.</p> </blockquote> <p><em>Solution.</em> (a) In this case, since we are working with Uniform[-1, 1] random variables, so we use Legendre polynomials, and therefore \(\Psi_i = P_n\). To compute the PCE coefficients, we write out the expansion:</p> \[Y = \sin(\pi X) = \sum_{n=0}^{\infty}c_n P_n(X).\] <p>For convenience, let‚Äôs just write out the first four terms of the PCE of \(Y\). Using the definition of the Legendre polynomials from earlier,</p> \[\begin{aligned} Y &amp;\approx c_0 P_0 (X) + c_1 P_1 (X) + c_2 P_2 (X) + c_3 P_3 (X) + \cdots \\ &amp;= c_0 \cdot 1 + c_1 \cdot X + c_2 \cdot \frac{1}{2}(3X^2-1) + c_3 \cdot \frac{1}{2}(5X^3-3X) + \cdots \end{aligned}\] <p>To compute \(c_n\), we can follow the formula:</p> \[c_n = \frac{\mathbb{E}[Y\Psi_i(\xi)]}{\mathbb{E}[\Psi_i^2(\xi)]} = \frac{1}{h_n}\cdot \frac{1}{2} \int_{-1}^{1} \sin(\pi x) P_n(x) dx,\] <p>where</p> \[h_n = \mathbb{E}[P_n^2] = \int_{-1}^{1} P_n(x) p_X(x) dx = \int_{-1}^{1} P_n^2(x) \frac{1}{2} dx = \frac{1}{2} \cdot \frac{2}{2n+1} = \frac{1}{2n+1}.\] <p>We know that a uniform distribution \(U[a,b]\) has PDF \(p_X(x) = \frac{1}{b-a}\). Thus</p> \[c_n = \frac{2n+1}{2}\int_{-1}^{1}\sin (\pi x) P_n(x) dx.\] <p>Now we can easily compute the coefficients!</p> \[\begin{aligned} c_0 &amp;= \int_{-1}^{1} \sin(\pi x) \cdot 1 dx = 0, \\ c_1 &amp;= \frac{3}{2} \int_{-1}^{1} \sin(\pi x) \cdot x dx = \frac{3}{\pi}, \\ c_2 &amp;= \frac{5}{2} \int_{-1}^{1} \sin(\pi x) \cdot \dfrac{1}{2}(3x^2-1) dx = 0, \\ c_3 &amp;= \frac{7}{2} \int_{-1}^{1} \sin(\pi x) \cdot \frac{1}{2}(5x^3-3x) dx = \frac{7}{\pi} - \frac{105}{\pi^3}. \\ \end{aligned}\] <p>Therefore our truncated expansion can be written as the following:</p> \[\boxed{\,Y \approx \frac{3}{\pi} P_1(X) + \biggl( \frac{7}{\pi} - \frac{105}{\pi^3} \biggr) P_3(X) + \cdots = \frac{3}{\pi} x + \biggl( \frac{7}{\pi} - \frac{105}{\pi^3}\biggr) \cdot \frac{1}{2} (5X^3 - 3X) + \cdots}\] <p>(b) We‚Äôve been asked to find \(\mathbb{E}[Y]\) and \(\text{Var}(Y)\). To sanity check, let‚Äôs first find these values using just probability theory so that we may check our answers later.</p> \[\begin{aligned} \mathbb{E}[Y] &amp;= \mathbb{E}[\sin(\pi X)] = \int_{\Omega} \sin(\pi x)\, p_X(x)\, dx = \int_{-1}^{1} \sin(\pi x) \cdot \frac{1}{2}\, dx = 0. \end{aligned}\] \[\begin{aligned} \mathrm{Var}(Y) &amp;= \mathbb{E}[\sin^2(\pi X)] - \mathbb{E}[\sin(\pi X)]^2 = \mathbb{E}[\sin^2(\pi X)] = \int_{-1}^{1} \sin^2(\pi x) \cdot \frac{1}{2}\, dx = \frac{1}{2}. \end{aligned}\] <p>Now lets try to verify this with PCE. To find the mean, we note that:</p> \[\boxed{\mathbb{E}[Y] = c_0 = 0.}\] <p>To find the variance, we use the formula from before:</p> \[\begin{aligned} \mathrm{Var}(Y) &amp;= \sum_{n=1}^{K} c_n^2\, \mathbb{E}[P_n^2] \\ &amp;= c_0^2 \mathbb{E}[P_0^2] + c_1^2 \mathbb{E}[P_1^2] + c_2^2 \mathbb{E}[P_2^2] + c_3^2 \mathbb{E}[P_3^2] + \cdots \end{aligned}\] <p>Since only the odd terms contribute,</p> \[\mathrm{Var}(Y) = c_1^2 h_1 + c_3^2 h_3 + \cdots = \left(\frac{3}{\pi}\right)^2 \cdot \frac{1}{3} + \left(\frac{7}{\pi} - \frac{105}{\pi^3}\right)^2 \cdot \frac{1}{7} + \cdots \approx \boxed{0.49561.}\] <p>This is obviously not exactly equal to the variance we found earlier (which was 0.5), but we can code up a Python simulation to see that it indeed converges.</p> <p><img src="pce_convergence.png" alt="Plot displaying the true and approximated solution of variance using PCE."/></p> <hr/> <blockquote> <p><strong>Problem 2</strong></p> <p>Consider the stochastic ODE:</p> \[\frac{du(t, \xi)}{dt} = \lambda(\xi) u(t,\xi) \quad u(0, \xi)=u_0(\xi)\] <p>where \(\lambda(\xi) = \lambda_0 + \lambda_1 \xi\) with \(\xi \sim \mathcal{N}(0,1)\), and \(\lambda_0\) and \(\lambda_1\) are constants.</p> <p>(a) Find the solution of this ODE analytically and expand it.<br/> (b) Expand the ODE directly.<br/> (c) Comment on the two approaches: are they equivalent after truncation?<br/> (d) Determine the expectations and variances, and comment on their values with respect to the sign of \(\lambda_0\) and \(\lambda_1\).</p> </blockquote> <p><em>Solution.</em><br/> (a) Because \(\xi\) is Gaussian, we will make use of the Hermite polynomials in this problem. For each realization \(\xi\), our solution will be</p> \[u(t, \xi) = u_0 e^{\lambda(\xi)t} = u_0e^{(\lambda_0 + \lambda_1 \xi)t}.\] <p>We can let \(u(t, \xi) = c_n \sum_{n=0}^\infty \text{He}_n (\xi)\), where \(\text{He}_n\) are the Hermite polynomials. Their coefficients can be found as follows</p> \[c_n(t) = \frac{u_0}{n!} e^{\lambda_0 t} (\lambda_1 t)^n e^{(\lambda_1 t)^2/2}.\] <p>Thus the truncated expansion of order 2 is given by</p> \[u(t, \xi) \approx u_0 \exp\left( \lambda_0 t + \frac{\lambda_1^2 t^2}{2} \right) \left[ 1+ (\lambda_1 t) \xi + \frac{(\lambda_1 t)^2}{2}(\xi^2 -1) \right].\] \[\boxed{ u(t, \xi) \approx u_0 \exp\left( \lambda_0 t + \frac{\lambda_1^2 t^2}{2} \right) \left[ 1+ (\lambda_1 t) \xi + \frac{(\lambda_1 t)^2}{2}(\xi^2 -1) \right]. }\] <p>(b) For the Galerkin method, let‚Äôs try to solve the system for \(N=2\) polynomial terms. We assume the expansion \(u(t, \xi) = c_0(t) + c_1(t)\text{He}_1(\xi) + c_2(t)\text{He}_2(\xi)\). With the recurrence relation identities, note that</p> \[\xi \text{He}_0 = \text{He}_1, \quad \xi \text{He}_1 = \text{He}_2 + \text{He}_0, \quad \xi \text{He}_2 = \text{He}_3 + 2\text{He}_1.\] <p>We plug into \(\frac{du (t, \xi)}{dt} = (\lambda_0 + \lambda_1 \xi) u(t, \xi)\). The left-hand side is then</p> \[\frac{du(t, \xi)}{dt} = \dot{c_0}\text{He}_0 + \dot{c_1}\text{He}_1 + \dot{c_2}\text{He}_2,\] <p>and the right-hand side is given by:</p> \[(\lambda_0 + \lambda_1 \xi) u = (\lambda_0 c_0 + \lambda_1 c_1) \text{He}_0 + (\lambda_0 c_1 + \lambda_1 c_0 + 2\lambda_1 c_2) \text{He}_1 + (\lambda_0 c_2 + \lambda_1 c_1) \text{He}_2 + (\lambda_1 c_2) \text{He}_3.\] <p>Now, we can match the coefficients of \(\text{He}_0\), \(\text{He}_1\), \(\text{He}_2\) and drop the \(\text{He}_3\) term (for the purposes of our truncation) to get the following:</p> \[\boxed{ \begin{aligned} \dot{c_0} &amp;= \lambda_0 c_0 + \lambda_1 c_1, \\ \dot{c_1} &amp;= \lambda_0 c_1 + \lambda_1 c_0 + 2\lambda_1 c_2, \\ \dot{c_2} &amp;= \lambda_0 c_2 + \lambda_1 c_1 \end{aligned} } \qquad \boxed{ \begin{aligned} c_0(0) = u_0, \quad c_1(0) &amp;= 0, \quad c_2(0) = 0 \end{aligned} }\] <p>In matrix form, this becomes</p> \[\begin{bmatrix} \dot{c_0}\\ \dot{c_1}\\ \dot{c_2} \end{bmatrix} = \begin{bmatrix} \lambda_0 &amp; \lambda_1 &amp; 0\\ \lambda_1 &amp; \lambda_0 &amp; 2\lambda_1\\ 0 &amp; \lambda_1 &amp; \lambda_0 \end{bmatrix} \begin{bmatrix} c_0 \\ c_1 \\ c_2 \end{bmatrix}.\] <p>Solving this 3 √ó 3 linear ODE system gives \(c_0 (t), c_1 (t), c_2(t)\); the approximation is</p> \[u(t, \xi) \approx c_0 (t) + c_1 (t) \text{He}_1 (\xi) + c_2(t) \text{He}_2 (\xi).\] <p>To do this explicitly, we write in matrix form, \(\dot{\mathbf{c}} = A \mathbf{c}\) with</p> \[A = \begin{bmatrix} \lambda_0 &amp; \lambda_1 &amp; 0 \\ \lambda_1 &amp; \lambda_0 &amp; 2\lambda_1 \\ 0 &amp; \lambda_1 &amp; \lambda_0 \end{bmatrix}.\] <p>We write \(A=\lambda_0 I + \lambda_1 B\) with</p> \[B = \begin{bmatrix} 0 &amp; 1 &amp; 0 \\ 1 &amp; 0 &amp; 2\\ 0 &amp; 1 &amp; 0 \end{bmatrix}.\] <p>We check that \(B^3 = 3B\) (hence the polynomial is \(x(x^2 -3)\)). Therefore</p> \[e^{tA} = e^{\lambda_0 t} e^{\lambda_1 t B} \left(I + \alpha(t) B + \beta(t) B^2 \right),\] <p>with</p> \[\alpha(t) = \frac{\sinh (\sqrt{3} \lambda_1 t)}{\sqrt{3}}, \quad \beta(t) = \frac{\cosh(\sqrt{3} \lambda_1 t) - 1}{3}.\] <p>Applying this to the initial vector \(\mathbf{c}(0)=(u_0, 0,0)^\top\) and using \(B(1,0,0)^\top = (0,1,0)^\top\), \(B^2(1,0,0)^\top =(1,0,1)^\top\), gives explicit coefficients:</p> \[\boxed{ \begin{aligned} c_0 (t) &amp;= u_0 e^{\lambda_0 t} \dfrac{2 + \cosh (\sqrt{3} \lambda_1 t)}{3}, \\ c_1 (t) &amp;= u_0 e^{-\lambda_0 t} \dfrac{\sinh (\sqrt{3} \lambda_1 t)}{\sqrt{3}}, \\ c_2 (t) &amp;= u_0 e^{\lambda_0 t} \dfrac{\cosh (\sqrt{3} \lambda_1 t) -1}{3}. \end{aligned} }\] <p>Thus the 3-term approximation is</p> \[u(t, \xi) \approx c_0(t) + c_1(t) \text{He}_1(\xi) + c_2(t) \text{He}_2 (\xi),\] <p>with \(c_0, c_1, c_2\) as above. As a sanity check: at \(t=0\), \(\cosh(0)=1\), \(\sinh(0) = 0\), so \(c_0(0) = u_0\), \(c_1(0) = c_2(0) =0\).)</p> <p>To quickly check the two-term case, if we kept only \(\text{He}_0, \text{He}_1\), the system \(\dot{c_0} = \lambda_0 c_0 + \lambda_1 c_1\), \(\dot{c_1} = \lambda_0 c_1 + \lambda_1 c_0\) has a closed form</p> \[c_0 (t) = u_0 e^{\lambda_0 t}\cosh(\lambda_1 t), \quad c_1(t) = u_0 e^{\lambda_0 t}\sinh(\lambda_1 t).\] <p>(c) Are the two approaches equivalent? \</p> <p>Asymptotically, yes; both solutions from pt.(a) and (b) give the same infinite series. However, for finite orders, the projection method truncates the exponential series while the Galerkin method truncates the polynomial chaos term. This is only true, however, due to the fact that our ODE is linear.</p> <p>(d) For the mean, we know that \(\mathbb{E}[u(t, \xi)] = c_0(t)\). Using the analytical solution, we get</p> \[\mathbb{E}[u(t, \xi)] = u_0 \mathbb{E}[e^{(\lambda_0 + \lambda_1 \xi)t}] = u_ e^{\lambda_0 t} \mathbb{E} [e^{\lambda_1 t \xi}] = u_0 e^{\lambda_0 t} e^{(\lambda_1 t)^2/2},\] <p>where we used the fact that \(\mathbb{E}[e^{a\xi}] = e^{a^2/2}\) for \(\xi \sim \mathcal{N}(0,1).\) Thus</p> \[\boxed{ \mathbb{E}[u, (t, \xi)] = u_0 \exp \left(\lambda_0 t + \frac{\lambda_1^2 t^2}{2} \right).}\] <p>For the variance, using the fact that \(\text{Var}(u(t, \xi)) = \sum_{n=1}^{\infty} c_n^2 n!\), we get</p> \[\begin{aligned} \text{Var}(u(t, \xi)) &amp;= \mathbb{E}[u^2] - \mathbb{E}[u]^2 \\ &amp;= u_0^2 e^{2\lambda_0 t} \mathbb{E}[e^{2\lambda_1 t \xi}] - u_0^2e^{2\lambda_0 t + \frac{\lambda_1^2t^2} {2}} \\ &amp;= u_0^2 e^{2\lambda_0 t + 2\lambda_1^2 t^2} - u_0^2e^{2\lambda_0 t + \lambda_1^2t^2}. \end{aligned}\] <p>Thus</p> \[\boxed{ \text{Var}(u(t, \xi)) = u_0^2 e^{2\lambda_0 t + \lambda_1^2 t^2}(e^{\lambda_1^2t^2}-1). }\] <p>Regarding the effect of signs on \(\lambda_0\) and \(\lambda_1\) with respect to the mean and variance, we observe:</p> <ul> <li>when \(\lambda_0 &gt; 0\): growth on average; both mean and variance grow exponentially</li> <li>when \(\lambda_0 &lt; 0\): decay on average but behavior depends on \(\lambda_1\) <ul> <li>if | \(\lambda_0\) | &gt; \(\lambda_1^2/t\): mean decays (stable on average)</li> <li>if | \(\lambda_0\) |&lt; \(\lambda_1^2 t/2\): mean can still grow (variance effect dominates)</li> </ul> </li> <li>when \(\lambda_1=0\): deterministic case where \(\text{Var}(u)=0\)</li> <li>because \(\lambda_1\) only appears as \(\lambda_1^2\) in mean and variance, the sign of it has no impact</li> <li>with high | \(\lambda_1\) |: there is high uncertainty; variance term \(e^{\lambda_1^2 t^2}\) dominates</li> </ul> <hr/> <h2 id="conceptual-understanding">Conceptual understanding</h2> <p>The coefficients of our polynomials are selected such that their expansion would be the orthogonal projection of \(u\) on the span of these basis functions. The choice of the polynomial family is particularly important; they MUST match the input distribution in order for things to converge nicely. Different distributions implies different weights which implies different orthogonal polynomial families. If misaligned, the coefficients would no longer be minimizing the mean square error. It is precisely because of this property of orthogonality that computing moments is nice with PCE.</p> <p>By formulation, PCE minimizes the root mean square error (RMSE). Mathematically, this is called the \(L^2\) norm and it represents the energy of a random variable (although more commonly called the variance). By minimizing the \(L^2\) error, we are finding the best approximation of the original random variable on average. Physically, this represents the average energy difference between the true and approximated solution. The con to this is that while your solution is meant to have low error on average across various random states, you are not optimizing for any one state in particular.</p> <p>If we expand this to an application example in climate, we could take the following hypothetical: given a task to model temperature fields across a region, we may (very likely) have variables for which have high uncertainty due to partial information. This could be variables like humidity, surface albedo, etc. If we treat each of these uncertainties a s a random variable, then we‚Äôd be able to build a PCE for the temperature field. I‚Äôm honestly not sure if this would work, but perhaps this means we wouldn‚Äôt need explicit detailed physics for our field. The PCE would act similar to a surrogate model and provide us with a way to quantify the uncertainty in our input variables and how those impact temperature behavior.</p> ]]></content><author><name></name></author><category term="math"/><summary type="html"><![CDATA[notes and worked-out examples]]></summary></entry><entry><title type="html">understanding the math behind VAEs</title><link href="https://valtsao17.github.io/blog/2025/kl/" rel="alternate" type="text/html" title="understanding the math behind VAEs"/><published>2025-08-09T00:32:13+00:00</published><updated>2025-08-09T00:32:13+00:00</updated><id>https://valtsao17.github.io/blog/2025/kl</id><content type="html" xml:base="https://valtsao17.github.io/blog/2025/kl/"><![CDATA[<p>In VAEs (as well as many other ML models), the Kullback-Leibler (KL) divergence is used to measure the ‚Äúcloseness‚Äù between two probability distributions. Attempting to understand this basic concept in a deeper manner, here I derive the closed-form expression for it between two Gaussians, where \(q(z)\) represents the approximate posterior and \(p(z)\) represents the prior.</p> <hr/> <p>Assume that they are univariate (one dimension) for simplicity, and simplify this form:</p> \[KL(q(z) || p(z))) = E_{q(z)} \left[log\dfrac{q(z)}{p(z)}\right] = \int_z q(z) log \dfrac{q(z)}{p(z)}dz,\] <p>or,</p> \[KL(N(\mu_q,\sigma_q^2)||N(\mu_p, \sigma_p^2)).\] <p>Let‚Äôs start with the definition of the KL divergence, given as:</p> \[\begin{equation*} KL(q(z) || p(z))) = E_{q(z)} \left[log\dfrac{q(z)}{p(z)}\right] = \int_z q(z) log \dfrac{q(z)}{p(z)}dz \tag{1} \end{equation*}\] <p>We know that \(p(z)\) and \(q(z)\) are normal distributions with general form of their PDF being</p> \[\mathcal{N} (z; \mu, \sigma^2) = \dfrac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\dfrac{(z-\mu)^2}{2\sigma^2} \right).\] <p>Now, we plug in the \(\mu\) and \(\sigma^2\) of \(p(z)\) and \(q(z)\) and take their ratio in order to try and match the form of the KL divergence in Eq. (1). Thus, we get: \(\begin{equation*} \frac{q(z)}{p(z)} = \frac{\frac{1}{\sqrt{2\pi\sigma_q^2}} \exp\left( -\frac{(z-\mu_q)^2}{2\sigma_q^2} \right)}{\frac{1}{\sqrt{2\pi\sigma_p^2}} \exp\left( -\frac{(z-\mu_p)^2}{2\sigma_p^2} \right)} = \sqrt{\frac{\sigma_p^2}{\sigma_q^2}} \exp\left( -\frac{(z-\mu_q)^2}{2\sigma_q^2} + \frac{(z-\mu_p)^2}{2\sigma_p^2} \right). \tag{2} \end{equation*}\)</p> <p>Taking the logarithm of Eq.(2) yields</p> \[\log\dfrac{q(z)}{p(z)} = \dfrac{1}{2} \log \dfrac{\sigma_p^2}{\sigma_q^2} - \dfrac{(z-\mu_q)^2}{2\sigma_q^2} + \dfrac{(z-\mu_p)^2}{2\sigma_p^2}.\] <p>Because we want to attain the KL divergence form, we need to integrate over the real numbers of the distribution of \(q(z)\). This is equivalent to taking the expectation with respect to \(q(z)\). Doing this results in the final form \(\begin{equation*} \text{KL}(q(z) \| p(z)) = \dfrac{1}{2} \log \dfrac{\sigma_p^2}{\sigma_q^2} + \dfrac{1}{2} \left[\dfrac{\mathbb{E}_q [ (z-\mu_p)^2]}{\sigma_p^2} - \dfrac{\mathbb{E}_q [ (z-\mu_q)^2]}{\sigma_q^2}\right]. \tag{3} \end{equation*}\)</p> <p>From the definition of variance, we know that: \(\begin{equation*} \mathbb{E}_q [ (z-\mu_q)^2] = \sigma_q^2 \tag{4} \end{equation*}\)</p> <p>In addition, because \(q(z)\) follows a normal distribution given by \(\mathcal{N}(\mu_q, \sigma_q^2)\), we can simplify as follows: \(\begin{align*} \mathbb{E}_q [ (z-\mu_p)^2] &amp;= \mathbb{E}_q [ (z^2 - 2z\mu_p + \mu_p^2] \\ &amp;= \mathbb{E}_q [z^2] - 2\mu_p \mathbb{E}_q [z] + \mathbb{E}_q [\mu_p^2] \\ &amp;= \sigma_q^2 + \mu_q^2 - 2\mu_p \mu_q + \mu_p^2 \\ &amp;= \sigma_q^2 + (\mu_q - \mu_p)^2 \tag{5} \end{align*}\)</p> <p>Plugging Eq.(4) &amp; Eq.(5) into Eq. (3) and simplifying, we get the desired closed form expression: \(\begin{align*} \boxed{\text{KL}(q(z) \| p(z)) = \dfrac{1}{2} \log \dfrac{\sigma_p^2}{\sigma_q^2} + \dfrac{1}{2} \left[\dfrac{\sigma_q^2 + (\mu_q - \mu_p)^2}{\sigma_p^2} - 1 \right].} \tag{6} \end{align*}\)</p> <p>Assuming that \(ùëù(ùëß)\) is \(ùëÅ(0,1)\) for simplicity, we can show that this simplifies to:</p> \[\begin{align*} KL(N(\mu_q, \sigma_q^2)||N(0,1)) = \frac{1}{2}\sigma_q^2 + \mu_q^2 -1 -\log\sigma_q^2. \end{align*}\] <p>For a standard normal distribution where \(p(z)\) is \(\mathcal{N}(0,1)\), we know that \(\mu_p=0\) and \(\sigma_p^2=1\). Plugging these into Eq.(6) above, we get:</p> \[\begin{align} \text{KL}(q(z) \mid\mid p(z)) = \dfrac{1}{2} \log \dfrac{1}{\sigma_q^2} + \dfrac{1}{2} (\sigma_q^2 + (\mu_q-0)^2 -1). \tag{7}\\ \end{align}\] <p>Since we know \(\log \dfrac{1}{\sigma_q^2} = -\log \sigma_q^2\), we can simplify Eq.(1) and obtain \(\begin{align} \boxed{\text{KL}(q(z) \| p(z)) = \dfrac{1}{2} (\sigma_q^2 + \mu_q^2 -1 - \log\sigma_q^2).} \tag{8}\\ \end{align}\)</p> <p>We know that the KL divergence is always non-negative, so it must be minimized when it‚Äôs equal to 0. In our case, using the expression derived in Eq.(8):</p> \[\text{KL}(\mathcal{N}(\mu_q,\sigma_q^2) \| \mathcal{N}(0,1))) = \dfrac{1}{2} (\sigma_q^2 + \mu_q^2 -1 - \log\sigma_q^2),\] <p>we notice that this is minimized precisely when</p> \[\sigma_q^2 =1 \quad \text{and} \quad \mu_q = 0,\] <p>which would give us a divergence of 0‚Äìin other words, when \(q(z)\) is a standard normal distribution. Therefore, \(q(z)\) must equal \(p(z)\).</p>]]></content><author><name></name></author><category term="ml"/><category term="ml"/><category term="theory"/><summary type="html"><![CDATA[deriving KL-divergence]]></summary></entry></feed>