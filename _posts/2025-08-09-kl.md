---
layout: post
title:  understanding the math behind VAEs
date: 2025-08-09 00:32:13
description: deriving KL-divergence 
tags: ml theory
categories: ml
tabs: true
---
In VAEs (as well as many other ML models), the Kullback-Leibler (KL) divergence is used to measure the "closeness" between two probability distributions. Attempting to understand this basic concept in a deeper manner, here I derive the closed-form expression for it between two Gaussians, where $$q(z)$$ represents the approximate posterior and $$p(z)$$ represents the prior.

---

Assume that they are univariate (one dimension) for simplicity, and simplify this form:
$$KL(q(z) || p(z))) = E_{q(z)} \left[log\dfrac{q(z)}{p(z)}\right] = \int_z q(z) log \dfrac{q(z)}{p(z)}dz,$$
or, $$KL(N(\mu_q,\sigma_q^2)||N(\mu_p, \sigma_p^2)).$$

Let's start with the definition of the KL divergence, given as:
$$\begin{equation*}
KL(q(z) || p(z))) = E_{q(z)} \left[log\dfrac{q(z)}{p(z)}\right] = \int_z q(z) log \dfrac{q(z)}{p(z)}dz
\tag{1}
\end{equation*} $$

We know that $$p(z)$$ and $$q(z)$$ are normal distributions with general form of their PDF being
$$\mathcal{N} (z; \mu, \sigma^2) = \dfrac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\dfrac{(z-\mu)^2}{2\sigma^2} \right). $$

Now, we plug in the $$\mu$$ and $$\sigma^2$$ of $$p(z)$$ and $$q(z)$$ and take their ratio in order to try and match the form of the KL divergence in Eq. (1). Thus, we get:
$$
\begin{equation*}
\frac{q(z)}{p(z)} = \frac{\frac{1}{\sqrt{2\pi\sigma_q^2}} \exp\left( -\frac{(z-\mu_q)^2}{2\sigma_q^2} \right)}{\frac{1}{\sqrt{2\pi\sigma_p^2}} \exp\left( -\frac{(z-\mu_p)^2}{2\sigma_p^2} \right)}
= \sqrt{\frac{\sigma_p^2}{\sigma_q^2}} \exp\left( -\frac{(z-\mu_q)^2}{2\sigma_q^2} + \frac{(z-\mu_p)^2}{2\sigma_p^2} \right).
\tag{2}
\end{equation*}
$$

Taking the logarithm of Eq.(2) yields
$$
\log\dfrac{q(z)}{p(z)} = \dfrac{1}{2} \log \dfrac{\sigma_p^2}{\sigma_q^2} - \dfrac{(z-\mu_q)^2}{2\sigma_q^2} + \dfrac{(z-\mu_p)^2}{2\sigma_p^2}.
$$

Because we want to attain the KL divergence form, we need to integrate over the real numbers of the distribution of $$q(z)$$. This is equivalent to taking the expectation with respect to $$q(z)$$. Doing this results in the final form
$$
\begin{equation*}
\text{KL}(q(z) \| p(z)) = \dfrac{1}{2} \log \dfrac{\sigma_p^2}{\sigma_q^2} + \dfrac{1}{2} \left[\dfrac{\mathbb{E}_q [ (z-\mu_p)^2]}{\sigma_p^2} - \dfrac{\mathbb{E}_q [ (z-\mu_q)^2]}{\sigma_q^2}\right].
\tag{3}
\end{equation*}
$$

From the definition of variance, we know that:
$$
\begin{equation*}
\mathbb{E}_q [ (z-\mu_q)^2] = \sigma_q^2 
\tag{4}
\end{equation*}
$$

In addition, because $$q(z)$$ follows a normal distribution given by $$\mathcal{N}(\mu_q, \sigma_q^2)$$, we can simplify as follows:
$$
\begin{align*}
\mathbb{E}_q [ (z-\mu_p)^2] &= \mathbb{E}_q [ (z^2 - 2z\mu_p + \mu_p^2] \\
                            &= \mathbb{E}_q [z^2] - 2\mu_p \mathbb{E}_q [z] + \mathbb{E}_q [\mu_p^2] \\
                            &= \sigma_q^2 + \mu_q^2 - 2\mu_p \mu_q + \mu_p^2 \\
                            &= \sigma_q^2 + (\mu_q - \mu_p)^2
\tag{5}
\end{align*}
$$

Plugging Eq.(4) & Eq.(5) into Eq. (3) and simplifying, we get the desired closed form expression:
$$
\begin{align*}
\boxed{\text{KL}(q(z) \| p(z)) = \dfrac{1}{2} \log \dfrac{\sigma_p^2}{\sigma_q^2} + \dfrac{1}{2} \left[\dfrac{\sigma_q^2 + (\mu_q - \mu_p)^2}{\sigma_p^2} - 1 \right].}
\tag{6}
\end{align*}
$$

Assuming that $$ùëù(ùëß)$$ is $$ùëÅ(0,1)$$ for simplicity, we can show that this simplifies to:

$$
\begin{align*}
KL(N(\mu_q, \sigma_q^2)||N(0,1)) = \frac{1}{2}\sigma_q^2 + \mu_q^2 -1 -\log\sigma_q^2.
\end{align*}$$

For a standard normal distribution where $$p(z)$$ is $$\mathcal{N}(0,1)$$, we know that $$\mu_p=0$$ and $$\sigma_p^2=1$$. Plugging these into Eq.(6) above, we get:

$$
\begin{align}
\text{KL}(q(z) \mid\mid p(z)) = \dfrac{1}{2} \log \dfrac{1}{\sigma_q^2} + \dfrac{1}{2} (\sigma_q^2 + (\mu_q-0)^2 -1).
\tag{7}\\
\end{align}
$$

Since we know $$\log \dfrac{1}{\sigma_q^2} = -\log \sigma_q^2$$, we can simplify Eq.(1) and obtain
$$
\begin{align}
\boxed{\text{KL}(q(z) \| p(z)) = \dfrac{1}{2} (\sigma_q^2 + \mu_q^2 -1 - \log\sigma_q^2).}
\tag{8}\\
\end{align}
$$

We know that the KL divergence is always non-negative, so it must be minimized when it's equal to 0. In our case, using the expression derived in Eq.(8):

$$
\text{KL}(\mathcal{N}(\mu_q,\sigma_q^2) \| \mathcal{N}(0,1))) = \dfrac{1}{2} (\sigma_q^2 + \mu_q^2 -1 - \log\sigma_q^2),
$$

we notice that this is minimized precisely when 

$$
\sigma_q^2 =1 \quad \text{and} \quad \mu_q = 0,
$$

which would give us a divergence of 0--in other words, when $$q(z)$$ is a standard normal distribution. Therefore, $$q(z)$$ must equal $$p(z)$$.
