---
layout: post
title:  intro to polynomial chaos expansion (pce)
date: 2025-11-25 00:32:13
description: notes and worked-out examples
tags: math
categories: 
tabs: true
---
<iframe src="{{ '/assets/pdfs/PCE_Exercise_VT.pdf' | relative_url }}" width="100%" height="600px"></iframe>

<!--
## Introduction
This section is more meant for me to summarize some of the things I read, as well as put some thought into digesting these concepts on my own. A lot of the ideas I jot down here are because I thought them necessary to know in order to complete the exercises that follow. Please feel free to skip over this section and proceed to the solutions below.

Polynomial chaos expansion (PCE) is a way for us to represent random variables in terms of (orthogonal) polynomials, similar to a Taylor/Maclaurin series or a Fourier series. This series gets truncated in practice to get accurate approximations that converge much faster than typical Monte Carlo simulations. They also allow for closed-form formulas for mean, variance, etc. unlike other forms of decompositions/expansions. Primarily, this has been used in uncertainty quantification and sensitivty analysis applications to easily summarize distributions that may have weird forms. Unlike in Principal Component Analysis (PCA), where the basis is data-dependent (ie. the eigenvectors of a covariance matrix), the basis for PCE can be chosen a-priori according to the input distribution.


## Some preliminaries and notes
### Orthogonality 
Before diving into PCE, it's important for us to briefly review some key concepts that will act as the building blocks of our understanding later. Let's take two random variables $X$ and $Y$ with finite variance. We define the **inner product** between the two as

$$
<X,Y> \, = \mathbb{E}[XY].
$$

This is just like the dot product for vectors, but for random variables.
- We say two random variables $$X$$ and $$Y$$ are **orthogonal** if 
    $$
    \mathbb{E}[XY]=0.
    $$
- This is also called **independence**; recall that orthogonality between two vector represents a right angle between the two. This interpretation works in $$\mathbb{R}^n$$, but in more general spaces (i.e. Hilbert spaces), this sort of geometric intuition falls apart.

Now recall that the $$L^2$$ norm of a random variable $$X$$ is 

$$
||X||_{L^2} = \sqrt{\mathbb{E}[X^2]}.
$$

- This is important because the $$L^2$$ norm we can say represents the energy of a random variable. Minimizing the $$L^2$$ error means that we find the best approximation of the original random variable on average. In other words, rather than working directly with the random variable, we often times in probability work with the $$L^2$$ norm of the random variable.


When we approximate a random variable $$Y$$ by a truncated series $$\hat{Y}$$, the $$L^2$$ error is 

$$
\text{Error}= ||Y - \hat{Y}||_{L^2} = \sqrt{\mathbb{E}[(Y - \hat{Y})^2]}.
$$

This is exactly the **root mean square error (RMSE).**

### PCE Minimizes $$L^2$$ Error
One way to think of the $$L^2$$ norm is to see it as measuring the *size* of a random variable. What we mean by this is that it gives us a way to answer the question "how far from zero is this random variable on average?" As a way to penalize large errors more heavily, we square the random variable and thus the value is always positive.

> [!EXAMPLE]
> Suppose we're measuring temperature fluctuations: \[X\] can be -2, 0, +2, with equal probability.
> 
> - The \( L^2 \) norm is \( \|X\|_{L^2} = \sqrt{\tfrac{1}{3}((-2)^2 + 0^2 + 2^2)} = \sqrt{\tfrac{8}{3}} \approx 1.63 \).
> - This tells us the "typical" temperature deviation is about 1.63 degrees.
>
> If we approximate \( X \) with \( \hat{X}=0 \) (always predicting zero), the error is
> \( \|X - \hat{X}\|_{L^2} = \|X\|_{L^2} \approx 1.63 \).
> A better approximation would give a smaller \( L^2 \) error.



The key to understanding why PCE minimizes the $$L^2$$ error is that orthogonal basis functions let us find the best coefficients independently. Let's clarify this a bit more. Say we want to approximate some random variable $Y$ using a combination of polynomial basis functions $$\Psi_0, \Psi_1, \Psi_2, \dots$$: 
\[
Y \approx \hat{Y} = c_0 \Psi_0 + c_1 \Psi_1 + c_2 \Psi_2 + \dots + c_P \Psi_P.
\]

What coefficients $$c_1$$ make $$$\hat{Y}$$ as close as possible to $$Y$$? To answer this question, we try to minimize the $L^2$ error of $$\hat{Y}$: $\mathbb{E}[(Y-\hat{Y})^2]$$. We expand this error to get 
\begin{align*}
\mathbb{E}[(Y-\hat{Y})^2] &= \mathbb{E} \left[ \left(Y - \sum_{i=0}^{P} c_i \Psi_i \right)^2 \right] \\
&= \mathbb{E}[Y^2] - 2\sum_{i=0}^{P} c_i \mathbb{E}[Y \Psi_i] + \sum_{i=0}^P c_i^2 \mathbb{E}[\Psi_i^2] + \sum_{i \neq j}c_i c_j \mathbb{E}[\Psi_i \Psi_j].
\end{align*}

Because we want this quantity to be as small as possible, notice that the way we can make it as \textit{small as possible} is if the cross-terms $\mathbb{E}[\Psi_i \Psi_j$ were zero! And if $\mathbb{E}[\Psi_i \Psi_j]=0$, then by definition $\Psi_i$ and $\Psi_j$ are orthogonal. Hence our quantity becomes 
\[
\mathbb{E}[(Y-\hat{Y})^2] = \mathbb{E}[Y^2] - 2\sum_{i=0}^{P} c_i \mathbb{E}[Y \Psi_i] + \sum_{i=0}^P c_i^2 \mathbb{E}[\Psi_i^2].
\]

To minimize this with respect to each $c_i$, we take the derivation and set it to zero:
\[
\frac{\partial}{\partial c_i} \mathbb{E}[(Y - \hat{Y})^2] = -2 \mathbb{E}[Y \Psi_i] + 2c_i \mathbb{E}[\Psi_i^2] = 0.
\]

This allows us to reach the following formula:
\[
\boxed{c_i = \frac{\mathbb{E}[Y \Psi_i]}{\mathbb{E}[\Psi_i^2]}.}
\]

Notice that each coefficient can be computed independently (thanks to orthogonality), and this choice of coefficients automatically minimizes the $L^2$ error. If the polynomials weren't orthogonal, we'd have to solve coupled equations for all coefficients simultaneously...which is much harder and less accurate.

### Orthogonal polynomials
Now we understand that PCE minimizes the $L^2$ error. So, why do we need \textit{orthogonal} polynomials?
Let's say we were trying to approximate a function using a basis of vectors, like sines and cosines in Fourier series. If our basis vectors are orthogonal, computing coeï¬ƒcients is easy; we can just project onto each basis element independently. However, if theyâ€™re not orthogonal, then weâ€™d need to solve a system of equations, which is very computationally expensive, and doesnâ€™t always give the unique best coefficients. The same principle applies here, with the added fact that orthogonality depends on the probability distribution now.

When polynomials are orthogonal, this means that 
\[
<P_n, P_m> = \int P_n(\xi) P_m(\xi) \rho(\xi) d\xi = 0 \quad \text{for} \quad n \neq m,
\]
where $\rho(\xi)$ is the probability density function (PDF). From this, we can see that even with the same functions but different distributions, we can get different orthogonality results.

There are a few classical orthogonal polynomials, like the Hermite polynomials (for Gaussian distribution), the Laguerre polynomials (for Gamma distribution), and the Jacobi polynomials (for Beta distribution). \cite{classicalpolynomials} If we are working with a distribution that doesnâ€™t match one of the named few, often we need to perform a transformation of random variables (i.e. via an inverse CDF
transform) to one that does match (for example, $U [a, b] \mapsto U [-1, 1]$). 
\paragraph{Legendre polynomials} 
A special case of the Jacobi is the Legendre polynomials, which are associated with the Uniform distribution on $[-1,1]$. They are the natural basis for functions defined in an interval with no preference for any particular value.

Speaking of which, our first exercise is defined on the Uniform distribution! The first few Legendre polynomials are defined by:
\begin{align*}
P_0(x) &= 1, \\
P_1(x) &= x, \\
P_2(x) &= \frac{1}{2}(3x^2 - 1), \\
P_3(x) &= \frac{1}{2}(5x^3 -3x), \\
P_4(x) &= \frac{1}{8}(35x^4 - 30x^2 + 3), \\
 &\vdots
\end{align*}

 
Rigorously, Legendre polynomials are orthogonal with respect to the uniform measure on $[-1, 1]$:
\[
\int_{-1}^{1} P_n(x) P_m(x) dx = \frac{2}{2n+1} \delta_{nm}. 
\]

They can be calculated by the following recurrence relation:
\[
(n+1)P_{n+1}(x) = (2n+1)xP_n(x) - nP_{n-1}(x),
\]
with $P_0(x)=1$ and $P_1(x)=x$. Alternatively though, they can be defined using Rodrigues' Formula, which gives us:
\[
P_n(x) = \frac{1}{2^n n!} \frac{d^n}{dx^n}[(x^2-1)^n].
\]

\paragraph{Hermite polynomials} 
Another set of orthogonal polynomials are the Hermite polynomials, associated with the standard Normal distribution $\mathcal{N}(0,1)$. They are the natural basis for Gaussian randomness, which is quite prevalent due to the Central Limit Theorem. \\ \\
\noindent 
For the ensuing notes, we will follow the probabilist's Hermite polynomials, $\text{He}_n$ as opposed to the physicist's $H_n$,\cite{garfken67:math} taking note that there does indeed exist two conventions. The first few Hermite polynomials are:
\begin{align*}
    \text{He}_0(x) &= 1, \\
    \text{He}_1(x) &= x, \\
    \text{He}_2(x) &= x^2-1, \\
    \text{He}_3(x) &= x^3 -3x, \\
    \text{He}_4(x) &= x^4 - 6x^2 +3, \\
    \text{He}_5(x) &= x^5 -10x^3 + 15x, \\
    &\vdots
\end{align*}

\noindent 
We see here that each polynomials has degree $n$ and removes the mean of $x^n$ under the Gaussian measure:
\[
\mathbb{E}[\text{He}_n(\xi)] =0 \quad \text{for} \,\, \xi \sim \mathcal{N}(0,1) \,\, \text{and} \,\, n \geq 1.
\]

\noindent 
Rigorously, Hermite polynomials are orthogonal with respect to the standard Gaussian measure:
\[
\mathbb{E}[\text{He}_n(\xi) \text{He}_m(\xi)] = \int_{-\infty}^\infty \text{He}_n (x) \text{He}_m (x) \frac{e^{-x^2/2}}{\sqrt{2\pi}}dx = n! \cdot \delta_{nm}
\]

\noindent
Note the normalization: $\mathbb{E}[\text{He}_n^2] = n!$ (meaning factorial growth). We calculate them with the following recurrence relation:
\[
\text{He}_{n+1}(x) = x \cdot \text{He}_n (x) -n \cdot \text{He}_{n-1} (x),
\]
with $\text{He}_0 (x) =1$ and $\text{He}_1 (x) =x$. Using Rodrigues' Formula again, we can find 
\[
\text{He}_n(x) = (-1)^n e^{x^2/2} \frac{d^n}{dx^n}e^{-x^2/2}.
\]

\noindent
There exists deeper connections to stochastic theory as for why Hermite polynomials represent Gaussians, but we won't get into that here. For more information, see the Focker-Plank equation,\cite{10.1090/S0025-5718-01-01365-5, khasi2022numerical} Wiener chaos,\cite{Russo2022} and optimal basis.\cite{hermitebasis}

\paragraph{Key Property:} If $\xi$ follows distribution with density $\rho(\xi)$, then the polynomials ${P_n(\xi)}$ satisfy
\[
\int P_n(\xi) P_m(\xi) \rho(\xi) d\xi = h_n \delta_{nm}
\]
where $\delta_{nm}$ is the Kronecker delta and $h_n$ is a normalization constant.

\begin{tcolorbox}[colframe=OliveGreen!30!white, colback=OliveGreen!5!white, coltitle=OliveGreen!99!white, title=\textbf{Main Ideas}]
\begin{itemize}%[leftmargin=1em]
  \renewcommand{\labelitemi}{$\Rightarrow$}
\item{For Legendre polynomials, the normalization constants are:
\[
h_n = \int_{-1}^{1} P_n (\xi) P_m (\xi) d\xi = \frac{2}{2n+1}.
\]
Note: this assumes $U[0,1]$! If not the case, we need to multiply by the PDF of the uniform distribution.}

\item{For Hermite polynomials, the normalization constants are: 
\[
h_n = \int_{-\infty}^\infty \text{He}_n (\xi) \text{He}_m (\xi) d\xi = \sqrt{2\pi}n! .
\]

Note: this assumes $\mathcal{N}(0,1)$. For non-standard Gaussian, we'd need to multiply by another normalization constant.}
\end{itemize}
\end{tcolorbox}


---

$$
\log\dfrac{q(z)}{p(z)} = \dfrac{1}{2} \log \dfrac{\sigma_p^2}{\sigma_q^2} - \dfrac{(z-\mu_q)^2}{2\sigma_q^2} + \dfrac{(z-\mu_p)^2}{2\sigma_p^2}.
$$

Because we want to attain the KL divergence form, we need to integrate over the real numbers of the distribution of $$q(z)$$. This is equivalent to taking the expectation with respect to $$q(z)$$. Doing this results in the final form
$$
\begin{equation*}
\text{KL}(q(z) \| p(z)) = \dfrac{1}{2} \log \dfrac{\sigma_p^2}{\sigma_q^2} + \dfrac{1}{2} \left[\dfrac{\mathbb{E}_q [ (z-\mu_p)^2]}{\sigma_p^2} - \dfrac{\mathbb{E}_q [ (z-\mu_q)^2]}{\sigma_q^2}\right].
\tag{3}
\end{equation*}
$$

From the definition of variance, we know that:
$$
\begin{equation*}
\mathbb{E}_q [ (z-\mu_q)^2] = \sigma_q^2 
\tag{4}
\end{equation*}
$$

In addition, because $$q(z)$$ follows a normal distribution given by $$\mathcal{N}(\mu_q, \sigma_q^2)$$, we can simplify as follows:
$$
\begin{align*}
\mathbb{E}_q [ (z-\mu_p)^2] &= \mathbb{E}_q [ (z^2 - 2z\mu_p + \mu_p^2] \\
                            &= \mathbb{E}_q [z^2] - 2\mu_p \mathbb{E}_q [z] + \mathbb{E}_q [\mu_p^2] \\
                            &= \sigma_q^2 + \mu_q^2 - 2\mu_p \mu_q + \mu_p^2 \\
                            &= \sigma_q^2 + (\mu_q - \mu_p)^2
\tag{5}
\end{align*}
$$

Plugging Eq.(4) & Eq.(5) into Eq. (3) and simplifying, we get the desired closed form expression:
$$
\begin{align*}
\boxed{\text{KL}(q(z) \| p(z)) = \dfrac{1}{2} \log \dfrac{\sigma_p^2}{\sigma_q^2} + \dfrac{1}{2} \left[\dfrac{\sigma_q^2 + (\mu_q - \mu_p)^2}{\sigma_p^2} - 1 \right].}
\tag{6}
\end{align*}
$$

Assuming that $$ð‘(ð‘§)$$ is $$ð‘(0,1)$$ for simplicity, we can show that this simplifies to:

$$
\begin{align*}
KL(N(\mu_q, \sigma_q^2)||N(0,1)) = \frac{1}{2}\sigma_q^2 + \mu_q^2 -1 -\log\sigma_q^2.
\end{align*}$$

For a standard normal distribution where $$p(z)$$ is $$\mathcal{N}(0,1)$$, we know that $$\mu_p=0$$ and $$\sigma_p^2=1$$. Plugging these into Eq.(6) above, we get:

$$
\begin{align}
\text{KL}(q(z) \mid\mid p(z)) = \dfrac{1}{2} \log \dfrac{1}{\sigma_q^2} + \dfrac{1}{2} (\sigma_q^2 + (\mu_q-0)^2 -1).
\tag{7}\\
\end{align}
$$

Since we know $$\log \dfrac{1}{\sigma_q^2} = -\log \sigma_q^2$$, we can simplify Eq.(1) and obtain
$$
\begin{align}
\boxed{\text{KL}(q(z) \| p(z)) = \dfrac{1}{2} (\sigma_q^2 + \mu_q^2 -1 - \log\sigma_q^2).}
\tag{8}\\
\end{align}
$$

We know that the KL divergence is always non-negative, so it must be minimized when it's equal to 0. In our case, using the expression derived in Eq.(8):

$$
\text{KL}(\mathcal{N}(\mu_q,\sigma_q^2) \| \mathcal{N}(0,1))) = \dfrac{1}{2} (\sigma_q^2 + \mu_q^2 -1 - \log\sigma_q^2),
$$

we notice that this is minimized precisely when 

$$
\sigma_q^2 =1 \quad \text{and} \quad \mu_q = 0,
$$

which would give us a divergence of 0--in other words, when $$q(z)$$ is a standard normal distribution. Therefore, $$q(z)-->